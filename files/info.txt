------ Building recommenders ------

### Two things needed: ###
- 'fit' function to train our model
- 'recommend' function that uses our model to recommend

### In order to evaluate our recommender we have to define: ###
- A splitting of the data in URM_train and URM_test
- An evaluation metric
- A function computing the evaluation for each user

### The splitting of the data is very important to ensure your algorithm is evaluated in a
    realistic scenario by using test it has never seen. ###



### Relevant vs. Recommended items ###
# Relevant items are already known in the data set
=> true rating ð‘Ÿð‘¢ð‘– is greater than a given threshold (e.g. 3.5)
Relevant item: Has a True/Actual rating >= 3.5
Irrelevant item: Has a True/Actual rating < 3.5

# Recommended items are generated by recommendation algorithm
=> estimated rating ð‘ŸÌ‚ ð‘¢ð‘– is greater than the threshold and if it is among the k highest estimated ratings (topK).
Recommended item: has a predicted rating >= 3.5
Not recommended item: Has a predicted rating < 3.5



### Similarity measure ###
Confidence on similarity value?  if no much support (data to rely on) the value is not reliable.

## K-Nearest Neighbors
top-k: for each item, tell no. of items to keep in the similarity matrix. Those with the k highest estimated ratings

## Evaluation metrics
# Precision at k is the proportion of recommended items in the top-k set that are relevant
Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k)


### Content-based ###
# Cosine similarity

### Collaborative filtering ###
# Item-based algorithm (URM x similarity[itemxitem]) => diff ways to define similarity
# User-based algorithm: compute similarity between users not items (Similarity[userxuser] x URM)

# Select highest values for each item from the similarity matrix?
All about the way matrix multiplication is performed
=> in item-based keep top-k for each column. In user-based keep top-k for each row
=> in the top k for an item it's very likely to have popular items



### Hyperparameters tuning ###
Once we have built our model we can play with its parameters. According to the dataset, they can affect
performance in different ways
(1) Number of neighbors: according to datasets it will impact in different ways
(2) Shrinkage: (i.e. support) has a much stronger impact.
(3) Similarity type

- Combine a parameter search with the two: (1) and (2) to ensure maximum recommendation quality

- Remark: Be careful, overfitting!
While a thorough parameter tuning might result in significantly higher MAP on your validation split,
it could have only marginally better or even worse MAP on the test set

## Feature weighting: TF
- IDF or BM25

### Validation step ###
At every time the split (train-test) changes, perform hyperparams tuning


