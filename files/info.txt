------ Building recommenders ------

Two things needed:
- 'fit' function to train our model
- 'recommend' function that uses our model to recommend

## In order to evaluate our recommender we have to define:
- A splitting of the data in URM_train and URM_test
- An evaluation metric
- A function computing the evaluation for each user

## The splitting of the data is very important to ensure your algorithm is evaluated in a
    realistic scenario by using test it has never seen.

## User Relevant vs. Recommended items
An item is considered relevant if its true rating 𝑟𝑢𝑖 is greater than a given threshold.
An item is considered recommended if its estimated rating 𝑟̂ 𝑢𝑖 is greater than the threshold,
and if it is among the k highest estimated ratings.

# Relevant items are already known in the data set
Relevant item: Has a True/Actual rating >= 3.5
Irrelevant item: Has a True/Actual rating < 3.5

# Recommended items are generated by recommendation algorithm
Recommended item: has a predicted rating >= 3.5
Not recommended item: Has a predicted rating < 3.5

Precision at k is the proportion of recommended items in the top-k set that are relevant
Precision@k = (# of recommended
items @k that are relevant) / (# of recommended items @k)




### Why is GlobalEffect performing worse than TopPop even
    if we are taking into account more information about the interaction?

The test data contains a lot of low rating interactions...
We are testing against those as well, but GlobalEffects is penalizing interactions with low rating
URMTest.data[URMTest.data<=2]


Return the top-N recommendation for each user from a set of predictions


### Parameters tuning
Once we have built our model we can play with its parameters. According to the dataset, they can affect
 performance in different ways
- Number of neighbors
- Shrinkage
- Similarity type


- Remark: Be careful, overfitting!
While a thorough parameter tuning might result in significantly higher MAP on your validation split,
it could have only marginally better or even worse MAP on the test set

# Feature weighting: TF
- IDF or BM25